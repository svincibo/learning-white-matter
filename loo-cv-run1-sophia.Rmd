---
title: "LOO CV, Run1 (Sophia's version)"
author: "DJM"
date: "17 August 2022"
output: 
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r, eval=FALSE, echo=TRUE}
# Required packages
install.packages("tidyverse")
install.packages("glmnet")
install.packages("lme4")
install.packages("remotes")
remotes::install_github("dajmcdon/sparsegl")
```

```{r data-loading}
library(glmnet)
library(tidyverse)
recog <- read_csv("wml_data_beh_recog_n60_20220607.csv")
wr <- read_csv("wml_data_beh_write_n60_20220607.csv")
tracts <- read_csv("wml_data_mri_tractprofiles_n60_run1_20220816.csv")
#tracts <- read_csv("wml_data_mri_tractprofiles_n59_run2_20220816.csv")

# use fractional anisotropy
fa <- tracts %>% dplyr::select(subID, tractname, nodeID, fa)
fa_sub <- fa %>%
  filter(
    str_detect(tractname, "SLF") |
      str_detect(tractname, "MDLF") |
      str_detect(tractname, "TPC") |
      str_detect(tractname, "pArc") |
      str_detect(tractname, "IFOF") |
      str_detect(tractname, "ILF") 
  ) %>%
  mutate(gr = case_when(
    str_detect(tractname, "SLF") ~ "Dorsal",
    str_detect(tractname, "IFOF") | str_detect(tractname, "ILF") ~ "Ventral",
    TRUE ~ "PVP"
  ))
rm(tracts, fa)
fa_wide_means <- fa_sub %>%
  filter(nodeID > 20, nodeID < 181) %>%
  group_by(subID, tractname) %>%
  summarise(fa = mean(fa, na.rm = TRUE)) %>%
  pivot_wider(names_from = tractname, values_from = fa)
fa_wide <- fa_sub %>%
  filter(nodeID > 20, nodeID < 181) %>%
  dplyr::select(-gr) %>%
  pivot_wider(names_from = c(tractname, nodeID), values_from = fa)

recog_resp <- recog %>%
  group_by(subID) %>%
  summarise(rt = mean(RT[acc == 1], na.rm=TRUE),
            acc = mean(acc == 1, na.rm=TRUE))
```

```{r, fig.show='hold', out.width="45%"}
# plot(recog_resp$acc, recog_resp$rt)
bad_outlier <- recog_resp$subID[recog_resp$acc <= .50]
```

Subject IDs `r bad_outlier` should be removed.

```{r kill-outlier}
missing_tracts <- c(25, 31, 32, 42) 
missing_run2 <- c(53) 
bad_testretest <- c(46, 50, 54, 56)
brain_anomaly <- c(60)
remove_ids <- c(bad_outlier, missing_tracts, brain_anomaly, missing_run2, bad_testretest)

fa_sub <- fa_sub %>% filter(! subID %in% remove_ids)
fa_wide <- fa_wide %>% filter(! subID %in% remove_ids)
fa_wide_means <- fa_wide_means %>% filter(! subID %in% remove_ids)
recog_resp <- recog_resp %>% filter(! subID %in% remove_ids)
wr <- wr %>% filter(! subID %in% remove_ids)

# grab the slopes
fits <- wr %>%
  group_by(subID) %>%
  group_modify( ~ {
    as.data.frame(summary(
      lme4::lmer(drawduration ~ block + (1 | stimulus), data=.x)
    )$coefficients[2,,drop=FALSE])
  })
all_resp <- full_join(recog_resp, fits) %>%
  dplyr::select(subID, rt, acc, Estimate) %>%
  rename(learn = Estimate) %>%
  mutate(learn = -learn) %>%
  filter()
```

# Relaxed lasso on means for selected tracks (no grouping)

```{r}
y <- as.matrix(all_resp %>% dplyr::select(-subID))
x <- fa_wide_means %>% 
  ungroup() %>% 
  dplyr::select(-subID) %>% 
  as.matrix()
```

```{r, warning=FALSE}
mod_acc <- cv.glmnet(x, y[,"acc"], relax = TRUE, gamma = 0, nfolds = nrow(x))
mod_learn <- cv.glmnet(x, y[,"learn"], relax = TRUE, gamma = 0, nfolds = nrow(x))

mean_model_lasso <- lapply(list(acc = mod_acc, learn = mod_learn),
                           coef, s = "lambda.min") %>%
  lapply(as.vector) %>%
  bind_cols() %>%
  mutate(var = c("(Intercept)", colnames(x))) %>%
  pivot_longer(-var) %>%
  filter(abs(value) > 0)

plot(mod_acc)
plot(mod_learn)
```

## Confidence intervals for this fit.

Because $n > p$, technically, the OLS intervals are valid. This is the simplest thing to do. So let's just use those.

```{r glmnet-cis}
lm_acc <- lm(acc ~. , bind_cols(acc = y[,"acc"], x))
lm_learn <- lm(learn ~ ., bind_cols(learn = y[,"learn"] , x))

mean_model_lasso <- left_join(
  mean_model_lasso,
  bind_rows(as_tibble(confint(lm_acc)) %>% 
              mutate(name = "acc", var = names(coef(lm_acc))),
            as_tibble(confint(lm_learn)) %>% 
              mutate(name = "learn", var = names(coef(lm_learn))))
  )

better_labels = c(acc = "Recognition Accuracy", learn = "Drawing Duration")
  
ggplot(mean_model_lasso, aes(y=var, color = var)) +
  geom_point(aes(x=value)) +
  geom_errorbar(aes(xmin = `2.5 %`, xmax = `97.5 %`)) +
  facet_wrap(~ name, scales = "free", labeller = labeller(name = better_labels)) +
  theme_bw() +
  geom_vline(xintercept = 0) +
  theme(legend.position = "none") +
  scale_color_viridis_d()
```

## See how well the regions for drawing (learning) predict accuracy

```{r}
# predsdraw <- predict(mod_learn, newx = x, s = mod_learn$lambda.min)
predsacc <- predict(mod_acc, newx = x, s = "lambda.min")
acc_model_with_draw_covariates <- 
  lm(acc ~ ., bind_cols(
    acc = y[,"acc"], 
    x[, predict(mod_learn, s="lambda.min", type = "nonzero")$lambda.min,
      drop=FALSE])) %>%
  predict(newx = x)

mse1 = mean((y[,"acc"] - predsacc)^2) # accuracy model for accuracy
mse2 = mean((y[,"acc"] - acc_model_with_draw_covariates)^2)

perc_worse <- (mse2 / mse1 - 1) * 100 
```

Doing this, the predictions using the drawing regions are about `r round(perc_worse)`% worse.

```{r plt-preds}
bind_cols(accuracy = y[,"acc"], 
          `drawing model` = acc_model_with_draw_covariates,
          `accuracy model` = predsacc) %>%
  pivot_longer(-accuracy, values_to = "predicted value") %>%
  ggplot(aes(x = `predicted value`, y = accuracy)) + 
  geom_point(aes(color = name, shape = name)) +
  theme_bw() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  geom_abline(slope = 1, intercept = 0)
```

## See how well the regions for accuracy predict drawing (learning)

```{r}
# predsdraw <- predict(mod_learn, newx = x, s = mod_learn$lambda.min)
predslearn <- predict(mod_learn, newx = x, s = "lambda.min")
learn_model_with_acc_covariates <- 
  lm(learn ~ ., bind_cols(
    learn = y[,"learn"], 
    x[, predict(mod_acc, s="lambda.min", type = "nonzero")$lambda.min,
      drop=FALSE])) %>%
  predict(newx = x)

mse1 = mean((y[,"learn"] - predslearn)^2) # accuracy, model for learn
mse2 = mean((y[,"learn"] - learn_model_with_acc_covariates)^2)

perc_worse <- (mse2 / mse1 - 1) * 100 
```

Doing this, the predictions using the accuracy regions are about `r round(perc_worse)`% worse.

```{r plt-preds2}
bind_cols(draw = y[,"learn"], 
          `drawing model` = predslearn,
          `accuracy model` = learn_model_with_acc_covariates) %>%
  pivot_longer(-draw, values_to = "predicted value") %>%
  ggplot(aes(x = `predicted value`, y = draw)) + 
  geom_point(aes(color = name, shape = name)) +
  theme_bw() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  geom_abline(slope = 1, intercept = 0)
```